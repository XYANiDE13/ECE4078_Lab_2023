# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
from YOLO.detector import Detector


# list of target fruits and vegs types
# Make sure the names are the same as the ones used in your YOLO model
TARGET_TYPES = ['orange', 'lemon', 'lime', 'tomato', 'capsicum', 'potato', 'pumpkin', 'garlic']


def estimate_pose(camera_matrix, obj_info, robot_pose):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {
        'orange': [0.083, 0.083, 0.075],
        'lemon': [0.055, 0.071, 0.054],
        'lime': [0.054, 0.074, 0.052],
        'tomato': [0.071, 0.069, 0.064],
        'capsicum': [0.0725, 0.077, 0.095],
        'potato': [0.076, 0.097, 0.06],
        'pumpkin': [0.086, 0.082, 0.074],
        'garlic': [0.066, 0.062, 0.072]
}
    #########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_center = target_box[0]
    distance = true_height/pixel_height * focal_length  # estimated distance between the object and the robot based on height
    # image size 640x480 pixels, 640/2=320
    x_shift = 320/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    horizontal_relative_distance = distance * np.sin(theta)     # relative distance between robot and object on x axis
    vertical_relative_distance = distance * np.cos(theta)       # relative distance between robot and object on y axis
    relative_pose = {'y': vertical_relative_distance, 'x': horizontal_relative_distance}    # relative object location

    ang = theta + robot_pose[2]     # angle of object in the world frame

    # location of object in the world frame
    target_pose = {'y': (robot_pose[1]+relative_pose['y']*np.sin(ang))[0],
                   'x': (robot_pose[0]+relative_pose['x']*np.cos(ang))[0]}

    return target_pose


# def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """
    target_est = {}

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    #threshold of 1 metre?
    threshold = 1
    #recognised fruits
    processed = set()

    #key 1 = current target, key 2 = next target
    for key1, pose1 in target_pose_dict.items():
        if key1 in processed:
            continue

        #determine current fruit
        fruit_type1 = None
        for fruit in TARGET_TYPES:
            if fruit in key1:
                fruit_type1 = fruit
                break
        if not fruit_type1:
            continue

        #position of current fruit
        x1, y1 = pose1['x'], pose1['y']
        count = 1
        total_x, total_y = x1, y1

        for key2, pose2 in target_pose_dict.items():
            if key1 == key2 or key2 in processed:
                continue

            #determine target fruit
            fruit_type2 = None
            for fruit in TARGET_TYPES:
                if fruit in key2:
                    fruit_type2 = fruit
                    break

            #compare current fruit with target fruit
            if fruit_type1 != fruit_type2:
                continue
            
            #position of target fruit
            x2, y2 = pose2['x'], pose2['y']
            distance = ((x2 - x1)**2 + (y2 - y1)**2)**0.5

            if distance <= threshold:
                count += 1
                total_x += x2
                total_y += y2
                processed.add(key2)

        avg_x = total_x / count
        avg_y = total_y / count
        target_est[key1] = {'x': avg_x, 'y': avg_y}
        processed.add(key1)
    #########
   
    return target_est


def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """
    target_est = {}

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)

    # Threshold for distance. If it is within the threshold, we will capture the X,Y coordinate
    threshold = 0.5
    already_processed = set() # Any processed label will be put into here. Remember set() = {...,...,...}
    x_list = []
    y_list = []
    # Run the loop over the target dictionary
    # label_key1 = current target label, label_key2 = next target label
    for label_key1, pose1 in target_pose_dict.items():
        if label_key1 in already_processed:
            continue # Skip the current iteration if the label has already been processed and/or duplicate

        label_key1_temp = label_key1[:len(label_key1)-2] # Delete 2 last characters, eg. orange_2 => orange for easier compare
        fruit_type1 = None
        if label_key1_temp in TARGET_TYPES:
            fruit_type1 = label_key1_temp
        else:
            continue

        # Position of the current fruit
        x1, y1 = pose1["x"], pose1["y"]
        x_list.append(x1)
        y_list.append(y1)

        for label_key2, pose2 in target_pose_dict.items():
            if label_key1 == label_key2 or label_key2 in already_processed:
                continue # Skip all of the already processed dictionary

            label_key2_temp = label_key2[:len(label_key2)-2]

            fruit_type2 = None
            if label_key2_temp in TARGET_TYPES:
                fruit_type2 = label_key2_temp
            else:
                continue

            if fruit_type1 != fruit_type2:
                continue
            
            x2, y2 = pose2["x"], pose2["y"]

            a = [x1, y1]
            b = [x2, y2]

            distance = np.linalg.norm(a-b)

            if distance <= threshold:
                x_list.append(x2)
                y_list.append(y2)
                already_processed.add(label_key2)

        # Median filter to smooth out the noise and errors
        result_median = median_filter(x_list=x_list, y_list=y_list, kernel_1D=3)

        # Average to reduce into 1 value from the median filter
        avg_x, avg_y = average_filter(result_median[:,0], result_median[:,1])

        target_est[label_key1] = {"y":avg_y, "x":avg_x}

        already_processed.add(label_key1)

        

    # target_est = target_pose_dict
    #########
   
    return target_est

def median_filter(x_list, y_list, kernel_1D = 3):

    # Check if the window size is odd (required for the median filter)
    if kernel_1D % 2 == 0:
        raise ValueError("Window size must be odd.")
    
    # Get the length of the input signal
    signal_length = len(x_list)

    x_list = np.array(x_list)
    y_list = np.array(y_list)

    # Create an empty array for the filtered output
    filtered_signals = np.zeros_like(np.column_stack((x_list, y_list)))

    # Iterate over the input signal
    for i in range(signal_length):
        # Extract the region of interest (ROI) within the window
        start = max(0, i - kernel_1D // 2)
        end = min(signal_length, i + kernel_1D // 2 + 1)
        roi_x = x_list[start:end]
        roi_y = y_list[start:end]
	
        # Calculate the median of the ROI and assign it to the corresponding pixel in the filtered signal
        filtered_signals[i,0] = np.median(roi_x)
        filtered_signals[i,1] = np.median(roi_y)

    return filtered_signals[1:-2,:]

def average_filter(x_list, y_list):
    return np.array(x_list).mean(), np.array(y_list).mean()


# main loop
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))     # get current script directory (TargetPoseEst.py)

    # read in camera matrix
    fileK = f'{script_dir}/calibration/param/intrinsic.txt'
    camera_matrix = np.loadtxt(fileK, delimiter=',')

    # init YOLO model
    model_path = f'{script_dir}/YOLO/model/yolov8_model.pt'
    yolo = Detector(model_path)

    # create a dictionary of all the saved images with their corresponding robot pose
    image_poses = {}
    with open(f'{script_dir}/lab_output/images.txt') as fp:
        for line in fp.readlines():
            pose_dict = ast.literal_eval(line)
            image_poses[pose_dict['imgfname']] = pose_dict['pose']

    # estimate pose of targets in each image
    target_pose_dict = {}
    detected_type_list = []
    for image_path in image_poses.keys():
        input_image = cv2.imread(image_path)
        bounding_boxes, bbox_img = yolo.detect_single_image(input_image)
        # cv2.imshow('bbox', bbox_img)
        # cv2.waitKey(0)
        robot_pose = image_poses[image_path]

        for detection in bounding_boxes:
            # count the occurrence of each target type
            occurrence = detected_type_list.count(detection[0])
            target_pose_dict[f'{detection[0]}_{occurrence}'] = estimate_pose(camera_matrix, detection, robot_pose)

            detected_type_list.append(detection[0])

    # merge the estimations of the targets so that there are at most 3 estimations of each target type
    target_est = {}
    target_est = merge_estimations(target_pose_dict)
    print(target_est)
    # save target pose estimations
    with open(f'{script_dir}/lab_output/targets.txt', 'w') as fo:
        json.dump(target_est, fo, indent=4)

    print('Estimations saved!')
